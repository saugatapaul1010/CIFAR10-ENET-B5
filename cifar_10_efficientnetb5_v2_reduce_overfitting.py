# -*- coding: utf-8 -*-
"""CIFAR_10_EfficientNetB5-V2-Reduce Overfitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Id0xfPUNd14NtRAXV2OnASEtBk1r7s6A

# Importing Libraries, Connect With G-Drive, Check GPU Availability
"""

#Importing all the libraries
from keras.backend import sigmoid
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from tensorflow.keras.applications import EfficientNetB5
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, Model, model_from_json
from tensorflow.keras.utils import to_categorical
import albumentations as albu
from sklearn.metrics import accuracy_score
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, GlobalMaxPooling2D, GlobalAveragePooling2D, LeakyReLU, SpatialDropout2D 
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History, TensorBoard, Callback, LearningRateScheduler
from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np
import math
from keras.models import load_model
import gc
import itertools
import os
import tempfile

# Commented out IPython magic to ensure Python compatibility.
#Mounting Gdrive
from google.colab import drive
drive.mount('/gdrive')
model_save_path = "/gdrive/My Drive/CIFAR-10-MODELS"
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)
# %cd /gdrive

#Check GPU Availability
print("TF version:", tf.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

os.environ["CUDA_VISIBLE_DEVICES"]="0"

"""# Helper Functions For Data Visualization and Data Transformations"""

def load_data():
    """
    Helper function to prepare the train, vaid, test datasets, for CIFAR-10.
    """
    (X, y), (X_test, y_test) = cifar10.load_data()
    st = StratifiedShuffleSplit(n_splits = 2, test_size = 0.2, random_state = 1947)

    #Splitting X into X_train, X_val. Keeping X_Test only for testing model's performance on unseen data.
    for train_index, val_index in st.split(X, y):
        X_train, X_val, y_train, y_val = X[train_index], X[val_index], y[train_index], y[val_index]

    print("The number of training data : ", X_train.shape[0])
    print("The number of validation data : ", X_val.shape[0])
    print("The number of test data : ", X_test.shape[0])

    del X, y

    return X_train, y_train, X_val, y_val, X_test, y_test

def plot_data(class_label_list):
    """
    Helper function to plot sample images from the training set.
    """
    NUM_CLASSES = len(class_label_list)
    class_plotted = np.random.choice(range(len(class_label_list)), NUM_CLASSES, replace = False)

    for i in range(len(class_plotted)):
        image_samples = X_train[y_train.reshape(-1) == class_plotted[i]][:10]
        fig, ax = plt.subplots(nrows = 1, ncols = 10,figsize = (8,8))
        fig.suptitle("Label : %d, Class : %s" % (class_plotted[i], class_label_list[class_plotted[i]]), y = .6)
        for j in range(10):
            ax[j].imshow(image_samples[j])
            ax[j].axis('off')  
        fig.tight_layout()
    plt.show()

    return NUM_CLASSES

# Converting class vectors to binary class matrices
def normalize_images(X_train, y_train, X_val, y_val, X_test, y_test):
    """
    Helper function to normalize the datasets"
    """
    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)
    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)
    y_val = keras.utils.to_categorical(y_val, NUM_CLASSES)

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_val /= 255
    X_test /= 255

    print("Images Normalized")

    return X_train, y_train, X_val, y_val, X_test, y_test

#Load Dataset CIFAR-10
X_train, y_train, X_val, y_val, X_test, y_test = load_data()

#Plot the data
class_label_list = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
NUM_CLASSES = plot_data(class_label_list)

X_train, y_train, X_val, y_val, X_test, y_test = normalize_images(X_train, y_train, X_val, y_val, X_test, y_test)

"""# Helper Functions Required For Model Training and Evaluation"""

class SwishActivation(Activation):
    """Custom Actvation Function. 
    Reference : https://iq.opengenus.org/swish-activation-function/, 
                https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820
    """    
    def __init__(self, activation, **kwargs):
        super(SwishActivation, self).__init__(activation, **kwargs)
        self.__name__ = 'swish_act'

def swish_act(x, beta = 1):
    return (x * sigmoid(beta * x))

get_custom_objects().update({'swish_act': SwishActivation(swish_act)})

class CosineAnnealingScheduler(Callback):
    """Cosine annealing scheduler Callback
    implemented using the Keras Callback class.
    References: https://paperswithcode.com/method/cosine-annealing
    """

    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):
        super(CosineAnnealingScheduler, self).__init__()
        self.T_max = T_max
        self.eta_max = eta_max
        self.eta_min = eta_min
        self.verbose = verbose

    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2
        K.set_value(self.model.optimizer.lr, lr)
        if self.verbose > 0:
            print('\nEpoch %05d: CosineAnnealingScheduler setting learning '
                  'rate to %s.' % (epoch + 1, lr))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)


def adjust_learning_rate(epoch):
    """This function is used if we want to manually decrease 
    the learning rate after specific epochs.
    """
    lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"]
    if epoch >= 10:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10
    if epoch >= 20:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100
    if epoch >= 30:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 1000
    if epoch >= 40:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10000
    if epoch >= 50:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100000
    if epoch >= 60:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 1000000
    if epoch >= 70:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10000000
    if epoch >= 80:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100000000
    return lr

def callbacks_list(input_params):
    """
    This function is used to define custom callbacks. Any new callbacks
    that are to be added to the model must be defined in this function
    and returned as a list of callbacks.
    """    
    base_checkpoint_folder = '/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_{}'.format(input_params["MODEL_NUMBER"])
    checkpoint_filepath = base_checkpoint_folder + '-{epoch:02d}-{val_accuracy:.2f}.h5'    
    checkpoint = ModelCheckpoint(checkpoint_filepath,
                                 monitor='val_accuracy',
                                 verbose=1,
                                 save_best_only=False,
                                 mode='max')
    
    reduce_learning_rate = ReduceLROnPlateau(monitor = input_params['REDUCE_LR_MONITOR'], 
                                             mode = input_params['REDUCE_LR_MODE'] , 
                                             patience = input_params['REDUCE_LR_PATIENCE'], 
                                             factor = input_params['REDUCE_LR_DECAY'], 
                                             min_lr = input_params['REDUCE_MIN_LR'], 
                                             verbose = 1)
    
    early_stop = EarlyStopping(monitor = input_params['EARLY_STOP_MONITOR'], 
                               mode = input_params['EARLY_STOP_MODE'], 
                               patience = input_params['EARLY_STOP_PATIENCE'], 
                               restore_best_weights = True, 
                               verbose = 1)
    
    history = History()

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="/gdrive/My Drive/tf_logs")

    #!tensorboard --logdir=/gdrive/My Drive/tf_logs/ --bind_all
    callbacks = [checkpoint, reduce_learning_rate, history, early_stop, tensorboard_callback]

    if input_params["LR_SCHEDULER"] == 'CosineAnnealing':
        callbacks.append(CosineAnnealingScheduler(T_max=input_params['TRAIN_EPOCHS'], eta_max=0.05, eta_min=4e-4))
    if input_params["LR_SCHEDULER"] == 'LearningRateScheduler':
        callbacks.append(LearningRateScheduler(adjust_learning_rate))
        
    return callbacks

def evaluate_model(model, X_test, y_test, X_train, y_train, X_val, y_val):
    """This function is used to evaluate the models
    performance across different datasets
    """
    loss, acc = model.evaluate(X_train, y_train)
    print("Training Accuracy: {}%".format(np.round(acc*100,2)))
    print("Training Loss: {}\n".format(np.round(loss,2)))

    loss, acc = model.evaluate(X_val, y_val)
    print("Validation Accuracy: {}%".format(np.round(acc*100,2)))
    print("Validation Loss: {}\n".format(np.round(loss,2)))

    loss, acc = model.evaluate(X_test, y_test)
    print("Test Accuracy: {}%".format(np.round(acc*100,2)))
    print("Test Loss: {}\n".format(np.round(loss,2)))


def plot_cm(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues, LABELS=class_label_list):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize=(12,12))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    target_names = LABELS

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)
    
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
def train_val_loss(history):
    """
    Function to check train vs validation loss/
    accuracy across all epochs.
    """
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs_range = range(1, len(history.epoch) + 1)

    plt.figure(figsize=(15,5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Train Set')
    plt.plot(epochs_range, val_acc, label='Val Set')
    plt.legend(loc="best")
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Efficient B5 Model Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Train Set')
    plt.plot(epochs_range, val_loss, label='Val Set')
    plt.legend(loc="best")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Efficient B5 Model Loss')

    plt.tight_layout()
    plt.show()

"""# Experiment : **1**

1. Changed the design of the fully connected layers, than the one used before
2. Used ImageDataGenerator for image augmentation- used roation, horizontal flip, width and height shift factors
3. Using a strong decay factor of 0.5 for ReduceLROnPlateau

### Define Model Hyperparameters:
"""

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.5
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 0.0000001
input_params['MINIBATCH_SIZE'] = 64
input_params['TRAIN_EPOCHS'] = 150
input_params['ACTIVATION_TYPE'] = "RELU" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 1 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.001
input_params["OPTIMIZER"] = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

"""### Define Model Architecture:"""

def build_model_1(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """

    conv_base = EfficientNetB5(include_top=False, input_shape=(32,32,3), weights='imagenet')

    #Setting al layers trainable == True
    for layer in conv_base.layers:
        layer.trainable = True

    activation_type = 'relu'

    # Adding fully-connected layers to B5.
    x = conv_base.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation=activation_type)(x) 
    x = layers.Dropout(0.7)(x)
    x = layers.Dense(256, activation=activation_type)(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation=activation_type)(x)
    x = layers.Dropout(0.2)(x)  

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)
    model_final = Model(inputs = conv_base.input, outputs = prediction_layer)

    # Define the optimizer and compile the model
    model_final.compile(loss='categorical_crossentropy', 
                        optimizer = input_params["OPTIMIZER"], 
                        metrics = input_params["EVAL_METRIC"])
    return model_final

model_1 = build_model_1(input_params)
model_1.summary()

"""### Define Model Augmentation Parameters:"""

#The below Datagen layer will be used for Image Augmentation
datagen = ImageDataGenerator(featurewise_center=False,
                            samplewise_center=False,
                            featurewise_std_normalization=False,
                            samplewise_std_normalization=False,
                            zca_whitening=False,
                            rotation_range=15,
                            width_shift_range=0.1,
                            height_shift_range=0.1,
                            horizontal_flip=True,
                            vertical_flip=False)

datagen.fit(X_train)

"""### Start Model Training:"""

#Start model training
history_1 = model_1.fit_generator(datagen.flow(X_train, y_train, batch_size = input_params['MINIBATCH_SIZE']),
                                  steps_per_epoch = len(X_train) // input_params['MINIBATCH_SIZE'], 
                                  epochs = input_params['TRAIN_EPOCHS'],
                                  callbacks=input_params["CALLBACKS"],
                                  validation_data= (X_val, y_val),
                                  verbose=1)

"""### Train vs Validation (Loss and Accuracy Graphs):"""

train_val_loss(history_1)

"""### Model Evaluation Across All Datasets:"""

best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_1-29-0.85.h5"
model_1 = load_model(best_model_path)
evaluate_model(model_1, X_test, y_test, X_train, y_train, X_val, y_val)

"""### Plot Confusion Matrix -- Training:"""

#Display the confusion matrix for train set
Y_pred = model_1.predict(X_train, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_train, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Validation:"""

#Display the confusion matrix for validation set
Y_pred = model_1.predict(X_val, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_val, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Unseen Test Data:"""

#Display the confusion matrix for test set
Y_pred = model_1.predict(X_test, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_test, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""# Experiment : **2**

1. Adding regularizer to the dense layers
2. Using proper weight initialization (he-normal for relu activation, glorot_normal for 'softmax' : This method is not giving good results)
3. Including ZOOM as another augmentation parameter
4. Training the model for longer duration so that the learning rate decreases adapatively during the later stages

### Define Model Hyperparameters:
"""

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 12
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 4
input_params['REDUCE_LR_DECAY'] = 0.5
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 0.0000001
input_params['MINIBATCH_SIZE'] = 64
input_params['TRAIN_EPOCHS'] = 150
input_params['ACTIVATION_TYPE'] = "RELU" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 2 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.001
input_params["OPTIMIZER"] = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

"""### Define Model Architecture:"""

def build_model_2(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """
    #kernel_regularizer = tf.keras.regularizers.l1(0.01)

    conv_base = EfficientNetB5(include_top=False, input_shape=(32,32,3), weights='imagenet')

    #Setting al layers trainable == True
    for layer in conv_base.layers:
        layer.trainable = True

    activation_type = 'relu'

    # Adding fully-connected with dropout layers to B5.
    x = conv_base.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation = activation_type)(x) 
    x = layers.Dropout(0.7)(x)
    x = layers.Dense(256, activation = activation_type)(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation = activation_type)(x)
    x = layers.Dropout(0.2)(x)  

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax", kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)
    model_final = Model(inputs = conv_base.input, outputs = prediction_layer)

    # Define the optimizer and compile the model
    model_final.compile(loss='categorical_crossentropy', 
                        optimizer = input_params["OPTIMIZER"], 
                        metrics = input_params["EVAL_METRIC"])
    return model_final

model_2 = build_model_2(input_params)
model_2.summary()

"""### Define Model Augmentation Parameters:"""

#The below ImageDataGenerator layer will be used for Image Augmentation
datagen = ImageDataGenerator(featurewise_center=False,
                            samplewise_center=False,
                            featurewise_std_normalization=False,
                            samplewise_std_normalization=False,
                            zca_whitening=False,
                            rotation_range=15,
                            zoom_range = 0.1,
                            width_shift_range=0.1,
                            height_shift_range=0.1,
                            horizontal_flip=True,
                            vertical_flip=False)

datagen.fit(X_train)

"""### Start Model Training:"""

#Start model training
history_2 = model_2.fit_generator(datagen.flow(X_train, y_train, batch_size = input_params['MINIBATCH_SIZE']),
                                  steps_per_epoch = len(X_train) // input_params['MINIBATCH_SIZE'], 
                                  epochs = input_params['TRAIN_EPOCHS'],
                                  callbacks=input_params["CALLBACKS"],
                                  validation_data= (X_val, y_val),
                                  verbose=1)

"""### Train vs Validation (Loss and Accuracy Graphs):"""

train_val_loss(history_2)

"""### Model Evaluation Across All Datasets:"""

best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_2-83-0.88.h5"
model_2 = load_model(best_model_path)

evaluate_model(model_2, X_test, y_test, X_train, y_train, X_val, y_val)

"""### Plot Confusion Matrix -- Training:"""

#Display the confusion matrix for train set
Y_pred = model_2.predict(X_train, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_train, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Validation:"""

#Display the confusion matrix for validation set
Y_pred = model_2.predict(X_val, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_val, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Unseen Test Data:"""

#Display the confusion matrix for test set
Y_pred = model_2.predict(X_test, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_test, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)





"""# Experiment : **3**

1. Same configuration as experiment 2. But here, we are adding regularization to all the fully connected layers instead of just the output layer.

### Define Model Hyperparameters:
"""

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 10
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 4
input_params['REDUCE_LR_DECAY'] = 0.5
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 0.0000001
input_params['MINIBATCH_SIZE'] = 64
input_params['TRAIN_EPOCHS'] = 150
input_params['ACTIVATION_TYPE'] = "RELU" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 3 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.001
input_params["OPTIMIZER"] = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

"""### Define Model Architecture:"""

def build_model_3(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """

    conv_base = EfficientNetB5(include_top=False, input_shape=(32,32,3), weights='imagenet')

    #Setting al layers trainable == True
    for layer in conv_base.layers:
        layer.trainable = True

    activation_type = 'relu'

    # Adding fully-connected layers to B5.
    x = conv_base.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation=activation_type, kernel_regularizer = tf.keras.regularizers.L2(0.01))(x) 
    x = layers.Dropout(0.7)(x)
    x = layers.Dense(256, activation=activation_type, kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation=activation_type, kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)
    x = layers.Dropout(0.2)(x)  

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax", kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)
    model_final = Model(inputs = conv_base.input, outputs = prediction_layer)

    # Define the optimizer and compile the model
    model_final.compile(loss='categorical_crossentropy', 
                        optimizer = input_params["OPTIMIZER"], 
                        metrics = input_params["EVAL_METRIC"])
    return model_final

model_3 = build_model_3(input_params)
model_3.summary()

"""### Define Model Augmentation Parameters:"""

#The below Datagen layer will be used for Image Augmentation
datagen = ImageDataGenerator(featurewise_center=False,
                             samplewise_center=False,
                             featurewise_std_normalization=False,
                             samplewise_std_normalization=False,
                             zca_whitening=False,
                             rotation_range=15,
                             zoom_range = 0.1,
                             width_shift_range=0.1,
                             height_shift_range=0.1,
                             horizontal_flip=True,
                             vertical_flip=False)

datagen.fit(X_train)

"""### Start Model Training:"""

#Start model training
history_3 = model_3.fit_generator(datagen.flow(X_train, y_train, batch_size = input_params['MINIBATCH_SIZE']),
                                  steps_per_epoch = len(X_train) // input_params['MINIBATCH_SIZE'], 
                                  epochs = input_params['TRAIN_EPOCHS'],
                                  callbacks=input_params["CALLBACKS"],
                                  validation_data= (X_val, y_val),
                                  verbose=1)

"""### Train vs Validation (Loss and Accuracy Graphs):"""

train_val_loss(history_3)

"""### Model Evaluation Across All Datasets:"""

#Load the saved model
best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_3-57-0.90.h5"
model_3 = load_model(best_model_path)

evaluate_model(model_3, X_test, y_test, X_train, y_train, X_val, y_val)

"""### Plot Confusion Matrix -- Training:"""

#Display the confusion matrix for train set
Y_pred = model_3.predict(X_train, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_train, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Validation:"""

#Display the confusion matrix for validation set
Y_pred = model_3.predict(X_val, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_val, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Unseen Test Data:"""

#Display the confusion matrix for test set
Y_pred = model_3.predict(X_test, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_test, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""# Experiment : **4**

1. Adding both L1 L2 to the dense fully connected layers as well as the efficient net convolution layers 
2. Adding a new fully connected layer and adjust the dropouts
3. Adding a very small decay factor in the Adam optimizer
4. Using a lower learning rate than previous experiments

### Define Model Hyperparameters:
"""

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 12
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 4
input_params['REDUCE_LR_DECAY'] = 0.5
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 0.0000001
input_params['MINIBATCH_SIZE'] = 128
input_params['TRAIN_EPOCHS'] = 150
input_params['ACTIVATION_TYPE'] = "RELU" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 4 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.005
input_params["OPTIMIZER"] = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

"""### Define Model Architecture:"""

def build_model_4(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """
    #kernel_regularizer = tf.keras.regularizers.l1(0.01)

    conv_base = EfficientNetB5(include_top=False, input_shape=(32,32,3), weights='imagenet')

    #Setting al layers trainable == True
    for layer in conv_base.layers:
        layer.trainable = True

    activation_type = 'relu'

    # Adding fully-connected with dropout layers to B5.
    x = conv_base.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation = activation_type)(x) 
    x = layers.Dropout(0.7)(x)
    x = layers.Dense(256, activation = activation_type)(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation = activation_type)(x)
    x = layers.Dropout(0.3)(x)  
    x = layers.Dense(64, activation = activation_type)(x)
    x = layers.Dropout(0.2)(x)  

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax", kernel_regularizer = tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4))(x)
    model_final = Model(inputs = conv_base.input, outputs = prediction_layer)

    # Define the optimizer and compile the model
    model_final.compile(loss='categorical_crossentropy', 
                        optimizer = input_params["OPTIMIZER"], 
                        metrics = input_params["EVAL_METRIC"])
    return model_final

def add_regularization(model, regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4)):
    """
    This function is used to add L1L2 penalty to both kernel_regularizer
    and bias_regularizer across all layers. When we change the layers 
    attributes, the change only happens in the model config file. Save 
    the weights before reloading the model. Load the model from the config.
    Reload the model weights.
    """

    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):
      print("Regularizer must be a subclass of tf.keras.regularizers.Regularizer")
      return model

    for layer in model.layers:
        for attr in ['kernel_regularizer', 'bias_regularizer']:
            if hasattr(layer, attr):
              setattr(layer, attr, regularizer)

    model_json = model.to_json()
    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')
    model.save_weights(tmp_weights_path)
    model = tf.keras.models.model_from_json(model_json)
    model.load_weights(tmp_weights_path, by_name=True)
    return model

def unfreeze_top_layers(model, input_params):
    # We unfreeze the top 100 layers while leaving base layers frozen
    for layer in model.layers[-500:]:
        if not isinstance(layer, layers.BatchNormalization):
            layer.trainable = True
    model.compile(optimizer=input_params["OPTIMIZER"], loss="categorical_crossentropy", metrics=["accuracy"])
    return model

model_4 = build_model_4(input_params)
#model_4 = unfreeze_top_layers(model_4, input_params)
model_4 = add_regularization(model_4) #Adding regularizer to the layers
model_4.compile(loss='categorical_crossentropy', optimizer = input_params["OPTIMIZER"], metrics = input_params["EVAL_METRIC"])
model_4.summary()

"""### Define Model Augmentation Parameters:"""

#The below ImageDataGenerator layer will be used for Image Augmentation
datagen = ImageDataGenerator(featurewise_center=False,
                            samplewise_center=False,
                            featurewise_std_normalization=False,
                            samplewise_std_normalization=False,
                            zca_whitening=False,
                            rotation_range=20,
                            zoom_range = 0.15,
                            width_shift_range=0.2,
                            height_shift_range=0.2,
                            horizontal_flip=True,
                            vertical_flip=True)

datagen.fit(X_train)

"""### Start Model Training:"""

#Start model training
history_4 = model_4.fit_generator(datagen.flow(X_train, y_train, batch_size = input_params['MINIBATCH_SIZE']),
                                  steps_per_epoch = len(X_train) // input_params['MINIBATCH_SIZE'], 
                                  epochs = input_params['TRAIN_EPOCHS'],
                                  callbacks=input_params["CALLBACKS"],
                                  validation_data= (X_val, y_val),
                                  verbose=1)

"""### Train vs Validation (Loss and Accuracy Graphs):"""

train_val_loss(history_4)

"""### Model Evaluation Across All Datasets:"""

best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_4-58-0.86.h5"
model_4 = load_model(best_model_path)

evaluate_model(model_4, X_test, y_test, X_train, y_train, X_val, y_val)

"""### Plot Confusion Matrix -- Training:"""

#Display the confusion matrix for train set
Y_pred = model_4.predict(X_train, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_train, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Validation:"""

#Display the confusion matrix for validation set
Y_pred = model_4.predict(X_val, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_val, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""### Plot Confusion Matrix -- Unseen Test Data:"""

#Display the confusion matrix for test set
Y_pred = model_4.predict(X_test, batch_size=input_params['MINIBATCH_SIZE'])
Y_pred_classes = np.argmax(Y_pred,axis=1) 
rounded_labels=np.argmax(y_test, axis=1)
cm_matrix = confusion_matrix(rounded_labels, Y_pred_classes)
plot_cm(cm_matrix, normalize=True, classes = range(len(class_label_list)), LABELS=class_label_list)

"""# Results"""

from prettytable import PrettyTable

myTable = PrettyTable(["Experiment Name", "Train Loss", "Validation Loss", "Test Loss", "Train Accuracy", "Validation Accuracy", "Test Accuracy" ])
myTable.add_row(["Experiment 1", "0.33", "0.45", "0.47", "89.2%", "85.34%", "85.11%"])
myTable.add_row(["Experiment 2", "0.17", "0.37", "0.39", "94.28%", "88.15%", "87.95%"])
myTable.add_row(["Experiment 3", "0.20", "0.47", "0.49", "97.36%", "89.52%", "88.94%"])
myTable.add_row(["Experiment 4", "1.18", "1.36", "1.37", "90.39", "85.65%", "84.55%"])  
print(myTable)

"""# Key Observations

1. For experiment 1, I have selected the model at the 29th epoch, after which the model accuracy on validation data remains constant. There is a significant improve in model performance from version 1. A 3.86% difference is observed between the train and validation set, and 4% between the train and test set. This indicates that the overfitting has been reduced considerably

2. For experiment 2, adding L2 regularizer to the last fully connected layers and tweaking the augmentation strategy by introducing zoom, increased all the train, validation and test accuracies. I have selected the model generated at epoch 83, post which there's no significant difference observed in model accuracy. There's a 6.13% difference in error observed between train and validation accuracies. This implies that overfitting has increased in experiment 2, from experiment 1.

3. For experiment 3, I have added regularization to all the fully connected layers and trained the model. The gap between train and validation error has now increased to almost 7%, which suggests significant overfitting than experiment 1.

4. For experiment 4, I have added both L1 and L2 regularizer for both the convolution layers as well as the convolution layers of efficient net. Tweaked the augmentation paparemeters more. I have also tried to freeze the convolution base and unfreeze only the top 20 layers. However, the accuracy was constant at 10% afte rmany epochs. Tried changing many hyperparameters, but there's no real effect. That's the reason I have not included the experiment in this notebook.

# Key Takeaway

If we look at all the experiments, experiment 1 has generated the least overfitted model after training for only 29 epochs (early stopped). The difference between train and validation dataset is 3.86%, which indicates slight overftting. On close observation we also notice that the train and validation loss are very very similar to each other (about 0.44 value for both losses), which indicates the model will generalize well on unseen data points. For a custom architecture, we could also have used SpatialDropout to tackle overfitting more. For the pre-trained architecture, I need more time and many more iterations to successfully integrate SpatialDropout.
"""