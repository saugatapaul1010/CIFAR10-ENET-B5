# -*- coding: utf-8 -*-
"""CIFAR-10-EfficientNetB5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gmp7HiJEBbIEysaY4dlvtlNjNjfPHNfq

# Importing Libraries, Connect With G-Drive, Check GPU Availability
"""

#Importing all the libraries
from keras.backend import sigmoid
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from tensorflow.keras.applications import EfficientNetB5
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import to_categorical
import albumentations as albu
from sklearn.metrics import accuracy_score
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, GlobalMaxPooling2D, GlobalAveragePooling2D, LeakyReLU
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History, TensorBoard, Callback, LearningRateScheduler
from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np
import math
from keras.models import load_model
import gc

# Commented out IPython magic to ensure Python compatibility.
#Mounting Gdrive
from google.colab import drive
drive.mount('/gdrive')
model_save_path = "/gdrive/My Drive/CIFAR-10-MODELS"
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)
# %cd /gdrive

#Check GPU Availability
print("TF version:", tf.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

os.environ["CUDA_VISIBLE_DEVICES"]="0"

"""# Helper Functions For Data Visualization and Data Transformations"""

def load_data():
    """
    Helper function to prepare the train, vaid, test datasets, for CIFAR-10.
    """
    (X, y), (X_test, y_test) = cifar10.load_data()
    st = StratifiedShuffleSplit(n_splits = 2, test_size = 0.2, random_state = 1947)

    #Splitting X into X_train, X_val. Keeping X_Test only for testing model's performance on unseen data.
    for train_index, val_index in st.split(X, y):
        X_train, X_val, y_train, y_val = X[train_index], X[val_index], y[train_index], y[val_index]

    print("The number of training data : ", X_train.shape[0])
    print("The number of validation data : ", X_val.shape[0])
    print("The number of test data : ", X_test.shape[0])

    del X, y

    return X_train, y_train, X_val, y_val, X_test, y_test

def plot_data(class_label_list):
    """
    Helper function to plot sample images from the training set.
    """
    NUM_CLASSES = len(class_label_list)
    class_plotted = np.random.choice(range(len(class_label_list)), NUM_CLASSES, replace = False)

    for i in range(len(class_plotted)):
        image_samples = X_train[y_train.reshape(-1) == class_plotted[i]][:10]
        fig, ax = plt.subplots(nrows = 1, ncols = 10,figsize = (8,8))
        fig.suptitle("Label : %d, Class : %s" % (class_plotted[i], class_label_list[class_plotted[i]]), y = .6)
        for j in range(10):
            ax[j].imshow(image_samples[j])
            ax[j].axis('off')  
        fig.tight_layout()
    plt.show()

    return NUM_CLASSES

# Converting class vectors to binary class matrices
def normalize_images(X_train, y_train, X_val, y_val, X_test, y_test):
    """
    Helper function to normalize the datasets"
    """
    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)
    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)
    y_val = keras.utils.to_categorical(y_val, NUM_CLASSES)

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_val /= 255
    X_test /= 255

    print("Images Normalized")

    return X_train, y_train, X_val, y_val, X_test, y_test

#Load Dataset CIFAR-10
X_train, y_train, X_val, y_val, X_test, y_test = load_data()

#Plot the data
class_label_list = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
NUM_CLASSES = plot_data(class_label_list)

X_train, y_train, X_val, y_val, X_test, y_test = normalize_images(X_train, y_train, X_val, y_val, X_test, y_test)

"""# Helper Functions Required For Model Training and Evaluation"""

class SwishActivation(Activation):
    """Custom Actvation Function. 
    Reference : https://iq.opengenus.org/swish-activation-function/, 
                https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820
    """    
    def __init__(self, activation, **kwargs):
        super(SwishActivation, self).__init__(activation, **kwargs)
        self.__name__ = 'swish_act'

def swish_act(x, beta = 1):
    return (x * sigmoid(beta * x))

get_custom_objects().update({'swish_act': SwishActivation(swish_act)})

class CosineAnnealingScheduler(Callback):
    """Cosine annealing scheduler Callback
    implemented using the Keras Callback class.
    References: https://paperswithcode.com/method/cosine-annealing
    """

    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):
        super(CosineAnnealingScheduler, self).__init__()
        self.T_max = T_max
        self.eta_max = eta_max
        self.eta_min = eta_min
        self.verbose = verbose

    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2
        K.set_value(self.model.optimizer.lr, lr)
        if self.verbose > 0:
            print('\nEpoch %05d: CosineAnnealingScheduler setting learning '
                  'rate to %s.' % (epoch + 1, lr))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)


def adjust_learning_rate(epoch):
    """This function is used if we want to manually decrease 
    the learning rate after specific epochs.
    """
    lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"]
    if epoch >= 10:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10
    if epoch >= 20:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100
    if epoch >= 30:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 1000
    if epoch >= 40:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10000
    if epoch >= 50:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100000
    if epoch >= 60:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 1000000
    if epoch >= 70:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 10000000
    if epoch >= 80:
        lr = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] / 100000000
    return lr

def callbacks_list(input_params):
    """
    This function is used to define custom callbacks. Any new callbacks
    that are to be added to the model must be defined in this function
    and returned as a list of callbacks.
    """    
    base_checkpoint_folder = '/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_{}'.format(input_params["MODEL_NUMBER"])
    checkpoint_filepath = base_checkpoint_folder + '-{epoch:02d}-{val_accuracy:.2f}.h5'    
    checkpoint = ModelCheckpoint(checkpoint_filepath,
                                 monitor='val_accuracy',
                                 verbose=1,
                                 save_best_only=True)
    
    reduce_learning_rate = ReduceLROnPlateau(monitor = input_params['REDUCE_LR_MONITOR'], 
                                             mode = input_params['REDUCE_LR_MODE'] , 
                                             patience = input_params['REDUCE_LR_PATIENCE'], 
                                             factor = input_params['REDUCE_LR_DECAY'], 
                                             min_lr = input_params['REDUCE_MIN_LR'], 
                                             verbose = 1)
    
    early_stop = EarlyStopping(monitor = input_params['EARLY_STOP_MONITOR'], 
                               mode = input_params['EARLY_STOP_MODE'], 
                               patience = input_params['EARLY_STOP_PATIENCE'], 
                               restore_best_weights = True, 
                               verbose = 1)
    
    history = History()

    #!tensorboard --logdir=/home/developer/Desktop/Saugata/logs/
    callbacks = [checkpoint, reduce_learning_rate, history, early_stop]

    if input_params["LR_SCHEDULER"] == 'CosineAnnealing':
        callbacks.append(CosineAnnealingScheduler(T_max=input_params['TRAIN_EPOCHS'], eta_max=0.05, eta_min=4e-4))
    if input_params["LR_SCHEDULER"] == 'LearningRateScheduler':
        callbacks.append(LearningRateScheduler(adjust_learning_rate))
        
    return callbacks

def start_training(input_params):
    """On calling this function, the model
    training will start"""
    model = input_params["MODEL"]
    model.compile(loss='categorical_crossentropy',
                optimizer=input_params["OPTIMIZER"],
                metrics=input_params["EVAL_METRIC"])

    hist = model.fit(X_train, y_train,
                batch_size=input_params['MINIBATCH_SIZE'],
                epochs=input_params['TRAIN_EPOCHS'],
                validation_data=(X_val, y_val),
                callbacks=input_params["CALLBACKS"],
                shuffle=True,
                verbose=1)
    return hist

def evaluate_model(model, X_test, y_test, dataset):
    """This function is used to evaluate the models
    performance across different datasets
    """
    _, acc = model.evaluate(X_test, y_test)
    print("{} Accuracy: {}%".format(dataset, acc*100))

def plot_cm(model, X, y):
    """Function to Plot Confustion Matrices.
    """
    pred = model.predict(X)
    classes = class_label_list
    ax = sns.heatmap(confusion_matrix(np.argmax(y, axis=1),np.argmax(pred, axis=1)), cmap="binary",annot=True,fmt="d")

def train_val_loss(hist):
    """
    Function to check train vs validation loss
    across all the epochs.
    """
    plt.figure(figsize=(20,12))
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train Loss', 'Val Loss'], loc='upper left')
    plt.show()

"""# Experiment : **1**

"""

def build_model_1(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """

    if(input_params['ACTIVATION_TYPE'] == "SWISH"):
        activation_type = swish_act
    elif(input_params['ACTIVATION_TYPE'] == "RELU"):
        activation_type = 'relu'

    base_model = EfficientNetB5(include_top=False, input_shape=(32,32,3), pooling='avg', weights='imagenet')

    # Adding 2 fully-connected layers to B5.
    x = base_model.output

    x = BatchNormalization()(x)
    x = Dropout(0.7)(x)

    x = Dense(512)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)
    x = Dropout(0.6)(x)

    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)

    model_final = Model(inputs = base_model.input, outputs = prediction_layer)
    return model_final

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.3
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 32
input_params['TRAIN_EPOCHS'] = 50
input_params['ACTIVATION_TYPE'] = "SWISH" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 1 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.00001

#This function is used to load the model architecture, for each different model architecture, we will create a new function.
input_params["MODEL"] = build_model_1(input_params)
input_params["OPTIMIZER"] = Adam(learning_rate = 0.00001, epsilon=1e-07, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_1 = input_params["MODEL"]
model_1.summary()

hist1 = start_training(input_params)

#Train this model for another 50 epochs and see how it performs.

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist1)

#Load the best model for this iteration
best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_1-33-0.71.h5"
model_1_final = load_model(best_model_path)

#Evaluate the loaded model across all datasets
evaluate_model(model_1_final, X_train, y_train, "Train")
evaluate_model(model_1_final, X_val, y_val, "Validation")
evaluate_model(model_1_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_1_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_1_final, X_train, y_train, class_label_list)

#Plot Validation Confusion Matrix
plot_cm(model_1_final, X_val, y_val, class_label_list)

#Removing previous memory allocations
del(input_params)
del(model_1)
del(model_1_final)
gc.collect()

"""# Experiment : **2**

In this experiment, we are keeping the initial configuration same (the one I have used for model 1), but this time introducing data augmentation. This is to validate, if adding augmentation layers will actually improve the models performance.
"""

def build_model_2(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass. For this function, I am adding
    an augmentation layer, and keeping all the hyparparameters same, to check how's 
    Augmentation affecting our models performance.
    """

    if(input_params['ACTIVATION_TYPE'] == "SWISH"):
        activation_type = swish_act
    elif(input_params['ACTIVATION_TYPE'] == "RELU"):
        activation_type = 'relu'

    #Augmentation layer
    img_augmentation = Sequential(
        [
            layers.RandomRotation(factor=0.15),
            layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
            layers.RandomFlip(),
            layers.RandomContrast(factor=0.1),
        ],
        name="img_augmentation",
    )

    inputs = layers.Input(shape=(32, 32, 3))
    x = img_augmentation(inputs)

    base_model = EfficientNetB5(include_top=False, input_tensor=inputs, pooling='avg', weights='imagenet')

    # Adding 2 fully-connected layers to B5.
    x = base_model.output

    x = BatchNormalization()(x)
    x = Dropout(0.7)(x)

    x = Dense(512)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)
    x = Dropout(0.6)(x)

    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)

    model_final = Model(inputs = base_model.input, outputs = prediction_layer)
    return model_final

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.3
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 32
input_params['TRAIN_EPOCHS'] = 100
input_params['ACTIVATION_TYPE'] = "SWISH" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 2 #ExperimentNumber

#This function is used to load the model architecture, for each different model architecture, we will create a new function.
input_params["MODEL"] = build_model_2(input_params)
input_params["OPTIMIZER"] = Adam(learning_rate = 0.00001, epsilon=1e-07, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_2 = input_params["MODEL"]
model_2.summary()

hist2 = start_training(input_params)

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist2)

#Load the best model for this iteration
best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_2-28-0.70.h5"
model_2_final = load_model(best_model_path)

#Evaluate the loaded model across all datasets
evaluate_model(model_2_final, X_train, y_train, "Train")
evaluate_model(model_2_final, X_val, y_val, "Validation")
evaluate_model(model_2_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_2_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_2_final, X_train, y_train)

#Plot Validation Confusion Matrix
plot_cm(model_2_final, X_val, y_val)

"""# Experiment : **3**

1. Changed Actvation Type to RELU
2. Changed number of epochs to 100, with early stopping.
"""

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.3
input_params['LR_SCHEDULER'] =  'None' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 32
input_params['TRAIN_EPOCHS'] = 100
input_params['ACTIVATION_TYPE'] = 'RELU' #"SWISH"
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 3 #ExperimentNumber

#Using model 1 architecture
input_params["MODEL"] = build_model_1(input_params)
input_params["OPTIMIZER"] = Adam(learning_rate = 0.00001, epsilon=1e-07, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_3 = input_params["MODEL"]
model_3.summary()

hist3 = start_training(input_params)

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist3)

#Load the best model for this iteration
best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_3-35-0.71.h5"
model_3_final = load_model(best_model_path)

#Evaluate the loaded model across all datasets
evaluate_model(model_3_final, X_train, y_train, "Train")
evaluate_model(model_3_final, X_val, y_val, "Validation")
evaluate_model(model_3_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_3_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_3_final, X_train, y_train)

#Plot Validation Confusion Matrix
plot_cm(model_3_final, X_val, y_val)

#Removing previous memory allocations
del(input_params)
del(model_3)
del(model_3_final)
gc.collect()

"""# Experiment : **4**

"""

def build_model_4(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We are also unfreezing only the top 25 layers,
    and keep the remaining layers frozen.
    """

    if(input_params['ACTIVATION_TYPE'] == "SWISH"):
        activation_type = swish_act
    elif(input_params['ACTIVATION_TYPE'] == "RELU"):
        activation_type = 'relu'
    elif(input_params['ACTIVATION_TYPE'] == "LeakyReLU"):
        activation_type = LeakyReLU(alpha=0.35)

    base_model = EfficientNetB5(include_top=False, input_shape=(32,32,3), pooling='avg', weights='imagenet')
    base_model.trainable = False

    # Adding 2 fully-connected layers to B5.
    x = base_model.output

    x = BatchNormalization()(x)
    x = Dropout(0.7)(x)

    x = Dense(512)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)
    x = Dropout(0.6)(x)

    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)
    model_final = Model(inputs = base_model.input, outputs = prediction_layer)
    model_final.compile(optimizer=input_params["OPTIMIZER"], loss="categorical_crossentropy", metrics=["accuracy"])
    return model_final

def unfreeze_model(model, input_params):
    # We unfreeze the top 25 layers while leaving BatchNorm layers frozen
    for layer in model.layers[-25:]:
        if not isinstance(layer, layers.BatchNormalization):
            layer.trainable = True
    model.compile(optimizer=input_params["OPTIMIZER"], loss="categorical_crossentropy", metrics=["accuracy"])
    return model

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_accuracy'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_accuracy'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.3
input_params['LR_SCHEDULER'] =  'LearningRateScheduler' #'LearningRateScheduler'  #'CosineAnnealing'
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 128
input_params['TRAIN_EPOCHS'] = 100
input_params['ACTIVATION_TYPE'] = "LeakyReLU" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 1 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.0005
input_params["OPTIMIZER"] = Adadelta(learning_rate = input_params["TRAIN_OPTIMIZER_LEARNING_RATE"])

#This function is used to load the model architecture, for each different model architecture, we will create a new function.
model = build_model_4(input_params)

#Ungfreezing the top layers
input_params["MODEL"] = unfreeze_model(model, input_params)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_4 = input_params["MODEL"]
model_4.summary()

hist4 = start_training(input_params)

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist4)

#Load the best model for this iteration
model_4_final = model_4

#Evaluate the loaded model across all datasets
evaluate_model(model_4_final, X_train, y_train, "Train")
evaluate_model(model_4_final, X_val, y_val, "Validation")
evaluate_model(model_4_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_4_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_4_final, X_train, y_train)

#Plot Validation Confusion Matrix
plot_cm(model_4_final, X_val, y_val)

#Removing previous memory allocations
del(input_params)
del(model_4)
del(model_4_final)
gc.collect()

"""# Experiment : **5**

In this experiment, we are keeping the initial configuration same (the one I have used for model 1), but this time introducing data augmentation. This is to validate, if adding augmentation layers will actually improve the models performance.
"""

def build_model_5(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass. For this function, I am adding
    an augmentation layer, and keeping all the hyparparameters same, to check how's 
    Augmentation affecting our models performance.
    """

    if(input_params['ACTIVATION_TYPE'] == "SWISH"):
        activation_type = swish_act
    elif(input_params['ACTIVATION_TYPE'] == "RELU"):
        activation_type = 'relu'

    base_model = EfficientNetB5(include_top=False, input_shape=(32,32,3), pooling='avg', weights='imagenet')

    # Adding 2 fully-connected layers to B5.
    x = base_model.output

    x = BatchNormalization()(x)
    x = Dropout(0.75)(x)

    x = Dense(1024)(x)
    x = BatchNormalization()(x)
    x = Activation(swish_act)(x)
    x = Dropout(0.80)(x)

    x = Dense(512)(x)
    x = BatchNormalization()(x)
    x = Activation(swish_act)(x)
    x = Dropout(0.75)(x)

    x = Dense(256)(x)
    x = BatchNormalization()(x)
    x = Activation(swish_act)(x)
    x = Dropout(0.70)(x)

    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation(swish_act)(x)

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)

    model_final = Model(inputs = base_model.input, outputs = prediction_layer)
    return model_final

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.3
input_params['LR_SCHEDULER'] =  "LearningRateScheduler"
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 32
input_params['TRAIN_EPOCHS'] = 100
input_params['ACTIVATION_TYPE'] = 'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 5 #ExperimentNumber
input_params["TRAIN_OPTIMIZER_LEARNING_RATE"] = 0.00001

#This function is used to load the model architecture, for each different model architecture, we will create a new function.
input_params["MODEL"] = build_model_5(input_params)
input_params["OPTIMIZER"] = Adam(learning_rate = 0.00001, epsilon=1e-07, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_5 = input_params["MODEL"]
model_5.summary()

hist5 = start_training(input_params)

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist5)

#Load the best model for this iteration --> Not loading the best model, because it's pointless
model_5_final = model_5

#Evaluate the loaded model across all datasets
evaluate_model(model_5_final, X_train, y_train, "Train")
evaluate_model(model_5_final, X_val, y_val, "Validation")
evaluate_model(model_5_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_5_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_5_final, X_train, y_train)

#Plot Validation Confusion Matrix
plot_cm(model_5_final, X_val, y_val)

"""# Experiment : 6"""

def build_model_6(input_params):
    """
    This function is used to build the model architecture. Here I am adding 2 
    fully-connected layers to B5. We will modify this function to change the 
    model architecture during each training pass.
    """

    if(input_params['ACTIVATION_TYPE'] == "SWISH"):
        activation_type = swish_act
    elif(input_params['ACTIVATION_TYPE'] == "RELU"):
        activation_type = 'relu'

    base_model = EfficientNetB5(include_top=False, input_shape=(32,32,3), pooling='avg', weights='imagenet')

    # Adding 2 fully-connected layers to B5.
    x = base_model.output

    x = BatchNormalization()(x)
    x = Dropout(0.75)(x)

    x = Dense(512)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)
    x = Dropout(0.65)(x)

    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation(activation_type)(x)

    # Output layer
    prediction_layer = Dense(input_params['NUM_CLASSES'], activation="softmax")(x)

    model_final = Model(inputs = base_model.input, outputs = prediction_layer)
    return model_final

#Define the Hyperparameters for each models
input_params = dict()
input_params['EARLY_STOP_MONITOR'] = 'val_loss'
input_params['EARLY_STOP_MODE'] = 'min'
input_params['EARLY_STOP_PATIENCE'] = 6
input_params['REDUCE_LR_MONITOR'] = 'val_loss'
input_params['REDUCE_LR_MODE'] = 'min'
input_params['REDUCE_LR_PATIENCE'] = 2
input_params['REDUCE_LR_DECAY'] = 0.4
input_params['LR_SCHEDULER'] =  'None'
input_params['REDUCE_MIN_LR'] = 1e-7
input_params['MINIBATCH_SIZE'] = 64
input_params['TRAIN_EPOCHS'] = 150
input_params['ACTIVATION_TYPE'] = "SWISH" #'RELU'
input_params['NUM_CLASSES'] = NUM_CLASSES
input_params["MODEL_NUMBER"] = 6 #ExperimentNumber

#This function is used to load the model architecture, for each different model architecture, we will create a new function.
input_params["MODEL"] = build_model_6(input_params)
input_params["OPTIMIZER"] = Adam(learning_rate = 0.0001, epsilon=1e-07, amsgrad=False)
input_params["CALLBACKS"] = callbacks_list(input_params)
input_params["EVAL_METRIC"] = 'accuracy'

#Instantiate the model and display the model summary
model_6 = input_params["MODEL"]
model_6.summary()

hist6 = start_training(input_params)

#As a first step for each experiment, we will visualize the train vs test loss.
train_val_loss(hist6)

#Load the best model for this iteration
best_model_path = "/gdrive/My Drive/CIFAR-10-MODELS/ENET_B5_Model_6-01-0.19.h5"
model_6_final = load_model(best_model_path)

#Evaluate the loaded model across all datasets
evaluate_model(model_6_final, X_train, y_train, "Train")
evaluate_model(model_6_final, X_val, y_val, "Validation")
evaluate_model(model_6_final, X_test, y_test, "TEST")

#Plot Test Confusion Matrix
plot_cm(model_6_final, X_test, y_test)

#Plot Train Confusion Matrix
plot_cm(model_6_final, X_train, y_train)

#Plot Validation Confusion Matrix
plot_cm(model_6_final, X_val, y_val)

#Removing previous memory allocations
del(input_params)
del(model_6_final)
del(model_6_final)
gc.collect()

"""#Key Observations:

Experiment 1 --

1. The train loss and validation loss reduces gradually (i.e. gradient updates are very smooth) and is seen to converge between epoch 27 and 32. Beyond epoch 32, the curves starts diverging which implies the model is getting overfitted to the train data.
2. Initially lower dropout rate was used which was later increased to have more aggressive dropout, to deal with overfitting.
3. Using a much much lower learning rate and training the model for more number of epochs will help further to reduce the most optimal solution. 
4. A partial overfit can be seen even when selecting the models between epoch 27 and 32.
5. More hyper parameter tuning needs to be done to improve the model's performance.

Experiment 2 --

1. Beyond epoch 30, the losses starts diverging and overfitting increases.
2. Optimal model is generated around epoch 30. 
3. Image augmentation improved the performance of the model very minutely.
4. Image Augmentation is slightly overfitting the model.

Experiment 3 --

1. Using the same architecture as model 1, but with RELU.
1. Here also a gradual decrease in train and  validation loss is seen, but the losses starts diverging beyond epoch 40.
2. Partial overfit is observed.
3. Gradient updates are not as smooth as Model with (Because of RELU as compared t SWISH)

Experiment 4, 5, 6 --

1. For 4 and 5, the validation loss remains constant around 2.32
2. Experiment with different optimizers, model architecture, learning rate, activation (leaky relu)
3. The losses doesn't converge at all.
4. For 6, the gradient updates are very nosiy, and the model have not converged at all across multiple tried for the last experiment 6.



Clearly all the top 3 performing models shows signs of partial overfitting. Few things that can be tried to reduce the overfitting much more is to reduce the learning rate combined with reduced batch size and train the model for significantly much larger epochs and check if it improves model performance as well as reduce model overfitting even more. Used cosine-annealing scheduler, reduce learning rate scheduler to deal with overfitting. We can also use Hough Transormation on the input images and check if it improves the model performance and reduces overfitting more. The scope for further experimentation is huge but it needs a very good compute engine and sufficient time to try out more techniques.

# Consolidated Results

![image.png](data:image/png;base64,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)
"""